# chat_with_qiita.py
import time

import streamlit as st
from langchain.agents import ConversationalChatAgent, AgentExecutor
from langchain.memory import ConversationBufferMemory
from langchain_community.callbacks import StreamlitCallbackHandler
from langchain_community.chat_message_histories import (
    StreamlitChatMessageHistory,
)

# from langchain_community.tools import DuckDuckGoSearchRun
# from langchain_community.tools import DuckDuckGoSearchResults
from langchain_core.runnables import RunnableConfig
from langchain_groq import ChatGroq
from langchain.tools import Tool
from langchain.prompts import PromptTemplate

from components.GropApiKey import GropApiKey
from components.ModelSelector import ModelSelector
from functions.GroqAPI import GroqAPI
from functions.QiitaApiItems import QiitaApiItems


def summarize_message(message: str, limit: int) -> str:
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¦ç´„ã™ã‚‹
    Args:
        message (str): ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        limit (int): ä¸Šé™æ–‡å­—æ•°
    Returns:
        str: è¦ç´„ã—ãŸæ–‡ç« 
    """
    # PromptTemplateã‚’ä½œæˆ
    prompt_template = PromptTemplate.from_template(
        "ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’{limit}æ–‡å­—ä»¥å†…ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚"
        "è¦ç´„ã¯æ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚\n\n"
        "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {message}\n\n"
        "è¦ç´„:"
    )
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…ˆé ­ã®5,000æ–‡å­—ã«åˆ¶é™
    truncated_message = message[:5000]
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
    prompt = prompt_template.format(message=truncated_message, limit=limit)
    # print(prompt)
    # é€šå¸¸ã®å›ç­”è¡¨ç¤º
    llm = GroqAPI(
        api_key=st.session_state.groq_api_key,
        model_name="llama-3.1-8b-instant",
    )
    summarized_message = llm.completion([{"role": "user", "content": prompt}])
    # print("## è¦ç´„æ–‡ï¼š")
    # print(summarized_message)
    return summarized_message


# ã‚«ã‚¹ã‚¿ãƒ æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã®å®šç¾©
def custom_search(query: str) -> str:
    exception_msg = """ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ç¾åœ¨æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã€‚
    åˆ¥ã®æ–¹æ³•ã§è³ªå•ã«ãŠç­”ãˆã—ã¾ã™ã®ã§ã€ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚
    """
    try:
        qiit_items = QiitaApiItems()
        # print(f"search query is {query}")
        time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–ã¨ã—ã¦1ç§’å¾…æ©Ÿ
        # search = DuckDuckGoSearchRun()
        # return search.run(query)
        # search = DuckDuckGoSearchResults()
        # return search.invoke(query)
        articles = qiit_items.get_articles(
            params={"query": query},
            page_size=5,
        )
        article_bodies = []
        for article in articles:
            article_body = article["body"]
            if len(article_body) >= 800:
                article_body = summarize_message(article_body, 800)
            article_bodies.append(f'# {article["title"]}\n{article_body}')
        return article_bodies
    except Exception as e:
        st.warning(f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return exception_msg


st.set_page_config(page_title="LangChain: Chat with search", page_icon="ğŸ”")
st.title("ğŸ”LangChain: Chat with Qiita search")
page_description = """ã“ã®ãƒšãƒ¼ã‚¸ã¯Qiita APIã§æ¤œç´¢ã—ãŸï¼•ã¤ã®çµæœã‹ã‚‰ã®å›ç­”ã—ã¾ã™ã€‚
æ¤œç´¢çµæœã¯è¦ç´„ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚‹ãŸã‚ã€ã™ã¹ã¦ã®æƒ…å ±ã‚’åˆ©ç”¨ã™ã‚‹ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚
"""
st.info(page_description)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š
st.sidebar.title("è¨­å®š")
# sidebar: apikey input
groq_api_key = GropApiKey()
groq_api_key.input_key()

# - model selector
model = ModelSelector()
with st.sidebar:
    st.subheader("Select a model, etc.")
    st.session_state.selected_model = model.select()

# ãƒ¡ã‚¤ãƒ³é ˜åŸŸã§ãƒãƒ£ãƒƒãƒˆ
if not st.session_state.groq_api_key:
    st.info("Groq API ã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¨­å®š
msgs = StreamlitChatMessageHistory()
memory = ConversationBufferMemory(
    chat_memory=msgs,
    return_messages=True,
    memory_key="chat_history",
    output_key="output",
)

if len(msgs.messages) == 0 or st.sidebar.button("ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ"):
    msgs.clear()
    msgs.add_ai_message("ã©ã®ã‚ˆã†ã«ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹ï¼Ÿ")
    st.session_state.steps = {}

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
for msg in msgs.messages:
    st.chat_message("user" if msg.type == "human" else "assistant").write(
        msg.content
    )
# avatars = {"human": "user", "ai": "assistant"}
# for idx, msg in enumerate(msgs.messages):
#     with st.chat_message(avatars[msg.type]):
#         # Render intermediate steps if any were saved
#         for step in st.session_state.steps.get(str(idx), []):
#             if step[0].tool == "_Exception":
#                 continue
#             with st.status(
#                 f"**{step[0].tool}**: {step[0].tool_input}", state="complete"
#             ):
#                 st.write(step[0].log)
#                 st.write(step[1])
#         st.write(msg.content)

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
    st.chat_message("user").code(prompt)

    # LLMã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨­å®š
    llm = ChatGroq(
        temperature=0,
        model=st.session_state.selected_model,
        api_key=st.session_state.groq_api_key,
    )
    # ã‚«ã‚¹ã‚¿ãƒ æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã®ä½œæˆ
    # tools = [DuckDuckGoSearchRun(name="Search")]
    search_description = """useful for answering questions about
    software technology, ICT related fields including computer software,
    web design, artificial intelligence, data analysis, cloud computing,
    cybersecurity, networking, databases and other related topics.
    """
    search_tool = Tool(
        name="Search", func=custom_search, description=search_description
    )
    tools = [search_tool]

    chat_agent = ConversationalChatAgent.from_llm_and_tools(
        llm=llm, tools=tools
    )
    executor = AgentExecutor.from_agent_and_tools(
        agent=chat_agent,
        tools=tools,
        memory=memory,
        return_intermediate_steps=True,
        handle_parsing_errors=True,
    )

    # å¿œç­”ã®ç”Ÿæˆã¨è¡¨ç¤º
    with st.chat_message("assistant"):
        st_cb = StreamlitCallbackHandler(
            st.container(), expand_new_thoughts=False
        )
        cfg = RunnableConfig()
        cfg["callbacks"] = [st_cb]
        response = executor.invoke(prompt, cfg)
        st.write(response["output"])
        st.session_state.steps[str(len(msgs.messages) - 1)] = response[
            "intermediate_steps"
        ]
